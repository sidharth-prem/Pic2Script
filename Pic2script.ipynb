{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidharth-prem/Pic2Script/blob/main/Pic2script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nv4HJsOm2Ux"
      },
      "outputs": [],
      "source": [
        "!pip install CocoDataset==0.1.2\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip /content/annotations_trainval2017.zip\n",
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!unzip /content/train2017.zip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip /content/val2017.zip\n",
        "!pip install pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR0Cew7cpbYf"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO # COCO python library\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "import random\n",
        "import string\n",
        "import cv2\n",
        "import os\n",
        "from pickle import dump, load\n",
        "import json\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Dropout, Attention\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import add\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGnSbC5XfGyN"
      },
      "outputs": [],
      "source": [
        "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
        "coco=COCO(\"../content/annotations/instances_train2017.json\")\n",
        "cats = coco.loadCats(coco.getCatIds())\n",
        "maincategories = list(set([cat['supercategory'] for cat in cats]))\n",
        "print(\"Number of main categories: \", len(maincategories))\n",
        "print(\"List of main categories: \", maincategories)\n",
        "subcategories = [cat['name'] for cat in cats]\n",
        "print(\"Number of sub categories: \", len(subcategories))\n",
        "print(\"List of sub categories: \", subcategories)\n",
        "catIds = coco.getCatIds(catNms=subcategories)\n",
        "subcategories_Ids = dict()\n",
        "for i in range(0,len(subcategories)):\n",
        "  subcategories_Ids[subcategories[i]] = catIds[i]\n",
        "print(\"Sub categories with IDs :\",subcategories_Ids)\n",
        "subcategories_imageIds = dict()\n",
        "for i in range(0,len(catIds)):\n",
        "  imgIds = coco.getImgIds(catIds=catIds[i])\n",
        "  img = []\n",
        "  for j in imgIds:\n",
        "    img.append(j)\n",
        "  subcategories_imageIds[subcategories[i]] = img\n",
        "print(\"Sub categories with Image IDs :\",len(subcategories_imageIds))\n",
        "length_dict = {key: len(value) for key, value in subcategories_imageIds.items()}\n",
        "print(\"Total images in each sub categories: \", length_dict)\n",
        "train_cats = subcategories_imageIds['bicycle'] + subcategories_imageIds['airplane']\n",
        "imgIdss = coco.getImgIds(imgIds = train_cats)\n",
        "print(\"Total Images: \", len(imgIdss))\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(16, 16)\n",
        "next_pix = imgIdss\n",
        "random.shuffle(next_pix)\n",
        "for i, img_path in enumerate(next_pix[0:12]):\n",
        "  sp = plt.subplot(4, 4, i + 1)\n",
        "  sp.axis('Off')\n",
        "  img = coco.loadImgs(img_path)[0]\n",
        "  I = io.imread(img['coco_url'])\n",
        "  plt.imshow(I)\n",
        "plt.show()\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(16, 16)\n",
        "for i, img_path in enumerate(next_pix[0:12]):\n",
        "  sp = plt.subplot(4, 4, i + 1)\n",
        "  sp.axis('Off')\n",
        "  img = coco.loadImgs(img_path)[0]\n",
        "  I = io.imread(img['coco_url'])\n",
        "  plt.imshow(I)\n",
        "  annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds,iscrowd=None)\n",
        "  anns = coco.loadAnns(annIds)\n",
        "  # print(anns)\n",
        "  coco.showAnns(anns)\n",
        "plt.show()\n",
        "annFile=\"../content/annotations/person_keypoints_train2017.json\"\n",
        "coco_kps=COCO(annFile)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(16, 16)\n",
        "for i, img_path in enumerate(next_pix[0:12]):\n",
        "  sp = plt.subplot(4, 4, i + 1)\n",
        "  sp.axis('Off')\n",
        "  img = coco.loadImgs(img_path)[0]\n",
        "  I = io.imread(img['coco_url'])\n",
        "  plt.imshow(I)\n",
        "  annIds = coco_kps.getAnnIds(imgIds=img['id'], catIds=catIds,iscrowd=None)\n",
        "  anns = coco_kps.loadAnns(annIds)\n",
        "  coco_kps.showAnns(anns)\n",
        "plt.show()\n",
        "annFile = \"../content/annotations/captions_train2017.json\"\n",
        "coco_caps=COCO(annFile)\n",
        "img = coco.loadImgs(next_pix[0])[0]\n",
        "I = io.imread(img['coco_url'])\n",
        "plt.imshow(I)\n",
        "annIds = coco_caps.getAnnIds(imgIds=img['id']);\n",
        "anns = coco_caps.loadAnns(annIds)\n",
        "coco_caps.showAnns(anns)\n",
        "plt.show()\n",
        "img = coco.loadImgs(next_pix[1])[0]\n",
        "I = io.imread(img['coco_url'])\n",
        "plt.imshow(I)\n",
        "annIds = coco_caps.getAnnIds(imgIds=img['id']);\n",
        "anns = coco_caps.loadAnns(annIds)\n",
        "coco_caps.showAnns(anns)\n",
        "plt.show()\n",
        "img = coco.loadImgs(next_pix[10])[0]\n",
        "I = io.imread(img['coco_url'])\n",
        "plt.imshow(I)\n",
        "annIds = coco_caps.getAnnIds(imgIds=img['id']);\n",
        "anns = coco_caps.loadAnns(annIds)\n",
        "coco_caps.showAnns(anns)\n",
        "plt.show()\n",
        "print(\"Total images for training: \", len(imgIdss))\n",
        "dataset = dict()\n",
        "imgcaptions = []\n",
        "\n",
        "for imgid in imgIdss:\n",
        "  img = coco.loadImgs(imgid)[0]\n",
        "  annIds = coco_caps.getAnnIds(imgIds=img['id']);\n",
        "  anns = coco_caps.loadAnns(annIds)\n",
        "  imgcaptions = []\n",
        "  for cap in anns:\n",
        "    # Remove punctuation\n",
        "    cap = cap['caption'].translate(str.maketrans('', '', string.punctuation))\n",
        "    # Replace - to blank\n",
        "    cap = cap.replace(\"-\",\" \")\n",
        "    # Split string into word list and Convert each word into lower case\n",
        "cap = cap.split()\n",
        "cap = [word.lower() for word in cap]\n",
        "# join word list into sentence and <start> and <end> tag to each sentence which helps\n",
        "# LSTM encoder-decoder model while training.\n",
        "cap = '<start> ' + \" \".join(cap) + ' <end>'\n",
        "imgcaptions.append(cap)\n",
        "\n",
        "dataset[img['coco_url']] = imgcaptions\n",
        "\n",
        "print(\"Length of Dataset: \",len(dataset))\n",
        "print(dataset[\"http://images.cocodataset.org/train2017/000000553573.jpg\"])\n",
        "\n",
        "#dataset\n",
        "from itertools import chain\n",
        "flatten_list = list(chain.from_iterable(dataset.values()))\n",
        "#[[1,3],[4,8]] = [1,3,4,8]\n",
        "tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\n",
        "tokenizer.fit_on_texts(flatten_list)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Vocabulary length: \", total_words)\n",
        "print(\"Bicycle ID: \", tokenizer.word_index['bicycle'])\n",
        "print(\"Airplane ID: \", tokenizer.word_index['airplane'])\n",
        "\n",
        "print(\"Image features length: \", len(image_features))\n",
        "image_features['http://images.cocodataset.org/train2017/000000047084.jpg'].shape\n",
        "def dict_to_list(descriptions):\n",
        "  all_desc = []\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "  return all_desc\n",
        "\n",
        "def max_length(descriptions):\n",
        "  desc_list = dict_to_list(descriptions)\n",
        "  return max(len(d.split()) for d in desc_list)\n",
        "\n",
        "max_length = max_length(dataset)\n",
        "max_length\n",
        "#create input-output sequence pairs from the image description.\n",
        "\n",
        "def data_generator(descriptions, features, tokenizer, max_length):\n",
        "  while 1:\n",
        "    for key, description_list in descriptions.items():\n",
        "      feature = features[key][0]\n",
        "    input_image, input_sequence, output_word =create_sequences(tokenizer, max_length, description_list, feature)\n",
        "    yield ([input_image, input_sequence], output_word)\n",
        "\n",
        "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
        "  X1, X2, y = list(), list(), list()\n",
        "  # walk through each description for the image\n",
        "  for desc in desc_list:\n",
        "    # encode the sequence\n",
        "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "    # split one sequence into multiple X,y pairs\n",
        "    for i in range(1, len(seq)):\n",
        "      # split into input and output pair\n",
        "      in_seq, out_seq = seq[:i], seq[i]\n",
        "      # pad input sequence\n",
        "      in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "      # encode output sequence\n",
        "      out_seq = to_categorical([out_seq],num_classes=total_words)[0]\n",
        "      # store\n",
        "      X1.append(feature) # image features\n",
        "      X2.append(in_seq) # Caption input\n",
        "      y.append(out_seq) # Caption output\n",
        "\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "from tensorflow.keras.utils import plot_model\n",
        "# define the captioning model\n",
        "def define_model(total_words, max_length):\n",
        "  # features from the CNN model squeezed from 2048 to 256 nodes\n",
        "  inputs1 = Input(shape=(2048,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256, activation='relu')(fe1)\n",
        "  # LSTM sequence model\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  se1 = Embedding(total_words, 256, mask_zero=True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "  # Merging both models\n",
        "  decoder1 = add([fe2, se3])\n",
        "  decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "  outputs = Dense(total_words, activation='softmax')(decoder2)\n",
        "  # tie it together [image, seq] [word]\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  # summarize model\n",
        "  print(model.summary())\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model\n",
        "\n",
        "# train our model\n",
        "print('Dataset: ', len(dataset))\n",
        "print('Descriptions: train=', len(dataset))\n",
        "print('Photos: train=', len(image_features))\n",
        "print('Vocabulary Size:', total_words)\n",
        "print('Description Length: ', max_length)\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "img_paths = [\"../content/val2017/000000001761.jpg\",\n",
        "\"../content/val2017/000000022396.jpg\" ,\n",
        "\"../content/val2017/000000098520.jpg\" ,\n",
        "\"../content/val2017/000000101762.jpg\" ,\n",
        "\"../content/val2017/000000224051.jpg\",\n",
        "]\n",
        "def extract_features(filename, model):\n",
        "  try:\n",
        "    image = Image.open(filename)\n",
        "  except:\n",
        "    print(\"ERROR: Couldn't open image! Make sure the image pathand extension is correct\")\n",
        "  image = image.resize((299,299))\n",
        "  image = np.array(image)\n",
        "\n",
        "  # for images that has 4 channels, we convert them into 3channels\n",
        "\n",
        "  if image.shape[2] == 4:\n",
        "    image = image[..., :3]\n",
        "  image = np.expand_dims(image, axis=0)\n",
        "  image = image/127.5\n",
        "  image = image - 1.0\n",
        "  feature = model.predict(image)\n",
        "  return feature\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == integer:\n",
        "      return word\n",
        "  return None\n",
        "\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "  in_text = 'start'\n",
        "  for i in range(max_length):\n",
        "    sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "    pred = model.predict([photo,sequence], verbose=0)\n",
        "    pred = np.argmax(pred)\n",
        "    word = word_for_id(pred, tokenizer)\n",
        "    if word is None:\n",
        "      break\n",
        "    in_text += ' ' + word\n",
        "    if word == 'end':\n",
        "      break\n",
        "  return in_text\n",
        "\n",
        "#max_length = 46\n",
        "#model = load_model('./models/model_0.h5')\n",
        "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
        "photo = extract_features(img_paths[0], xception_model)\n",
        "img = Image.open(img_paths[0])\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)\n",
        "photo = extract_features(img_paths[1], xception_model)\n",
        "img = Image.open(img_paths[1])\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)\n",
        "photo = extract_features(img_paths[2], xception_model)\n",
        "img = Image.open(img_paths[2])\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)\n",
        "photo = extract_features(img_paths[3], xception_model)\n",
        "img = Image.open(img_paths[3])\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)\n",
        "photo = extract_features(img_paths[4], xception_model)\n",
        "img = Image.open(img_paths[4])\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}